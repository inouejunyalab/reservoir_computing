{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. データの前処理及び変数、関数の宣言"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "sentences = [\"i like dogs\", \"i love coffee\", \"i hate milk\", \"you like cats\", \"you love milk\", \"you hate coffee\"]\n",
    "dtype = torch.float\n",
    "\n",
    "\"\"\"\n",
    "Word Processing\n",
    "\"\"\"\n",
    "word_list = list(set(\" \".join(sentences).split()))\n",
    "word_dict = {w: i for i, w in enumerate(word_list)}\n",
    "number_dict = {i: w for i, w in enumerate(word_list)}\n",
    "n_class = len(word_dict)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "TextRNN Parameter\n",
    "\"\"\"\n",
    "batch_size = len(sentences)\n",
    "n_hidden = 5\n",
    "\n",
    "def make_batch(sentences):\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "\n",
    "    for sen in sentences:\n",
    "        word = sen.split()\n",
    "        input = [word_dict[n] for n in word[:-1]]\n",
    "        target = word_dict[word[-1]]\n",
    "\n",
    "        input_batch.append(np.eye(n_class)[input])  # One-Hot Encoding\n",
    "        target_batch.append(target)\n",
    "  \n",
    "    return input_batch, target_batch\n",
    "\n",
    "input_batch, target_batch = make_batch(sentences)\n",
    "input_batch = torch.tensor(input_batch, dtype=torch.float32, requires_grad=True)\n",
    "target_batch = torch.tensor(target_batch, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. LSTMの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextLSTM, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=n_class, hidden_size=n_hidden, dropout=0.3)\n",
    "        self.W = nn.Parameter(torch.randn([n_hidden, n_class]).type(dtype))\n",
    "        self.b = nn.Parameter(torch.randn([n_class]).type(dtype))\n",
    "        self.Softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, hidden_and_cell, X):\n",
    "        X = X.transpose(0, 1)\n",
    "        outputs, hidden = self.lstm(X, hidden_and_cell)\n",
    "        outputs = outputs[-1]\n",
    "        model = torch.mm(outputs, self.W) + self.b\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoheokkim/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0100 cost = 0.230437\n",
      "Epoch: 0200 cost = 0.031391\n",
      "Epoch: 0300 cost = 0.013321\n",
      "Epoch: 0400 cost = 0.007688\n",
      "Epoch: 0500 cost = 0.005116\n"
     ]
    }
   ],
   "source": [
    "model = TextLSTM()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(500):\n",
    "    hidden = torch.zeros(1, batch_size, n_hidden, requires_grad=True)\n",
    "    cell = torch.zeros(1, batch_size, n_hidden, requires_grad=True)\n",
    "    output = model((hidden, cell), input_batch)\n",
    "    loss = criterion(output, target_batch)\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "  \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D. 検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'like'], ['i', 'love'], ['i', 'hate'], ['you', 'like'], ['you', 'love'], ['you', 'hate']] -> ['dogs', 'coffee', 'milk', 'cats', 'milk', 'coffee']\n"
     ]
    }
   ],
   "source": [
    "hidden = torch.zeros(1, batch_size, n_hidden, requires_grad=True)\n",
    "cell = torch.zeros(1, batch_size, n_hidden, requires_grad=True)\n",
    "predict = model((hidden, cell), input_batch).data.max(1, keepdim=True)[1]\n",
    "print([sen.split()[:2] for sen in sentences], '->', [number_dict[n.item()] for n in predict.squeeze()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-2.4433,  1.6995, -0.6711, -1.3300, -3.4730,  2.2199, -1.1504,  2.5870,\n",
      "         -1.0595],\n",
      "        [-0.8545, -0.8071, -0.5162,  2.5201, -1.1936, -2.3645,  0.1325,  2.4949,\n",
      "         -0.8585],\n",
      "        [ 1.4986, -3.7473,  0.1965, -1.1459,  1.3352,  1.8019,  2.3007,  1.5746,\n",
      "          0.6256],\n",
      "        [-1.3062,  2.1798,  0.8306, -2.7042, -1.2823, -2.3491, -0.8534,  1.6816,\n",
      "          0.9794],\n",
      "        [-0.8011,  1.5705, -2.0520,  1.4748, -1.0428, -2.8994, -0.8618, -2.3974,\n",
      "         -0.7201]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.6973, -1.2723,  0.8722,  0.7686,  0.4770,  0.2070,  0.1465,  0.2058,\n",
      "         0.1060], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.2907, -0.0568,  1.1087, -0.1635, -0.1776,  0.2999, -0.6543,  0.2586,\n",
      "          0.4940],\n",
      "        [ 0.5337,  0.0792,  0.8622, -0.3191,  0.7686,  0.1982,  0.7354,  0.1778,\n",
      "          0.2376],\n",
      "        [ 0.7075,  0.1099,  0.2279, -0.1378,  1.0445, -0.1929,  0.3203,  0.1997,\n",
      "         -0.5867],\n",
      "        [ 0.9616, -0.3859,  1.3631,  0.1873, -2.1405,  0.4050,  0.9109,  0.2609,\n",
      "          1.1973],\n",
      "        [ 0.8061,  0.1237,  1.1868, -0.2247,  0.8608,  0.1422,  0.1894,  0.0892,\n",
      "          0.0052],\n",
      "        [ 0.3580, -0.4467, -0.3105, -0.3757, -1.3770, -0.4455,  0.6492, -0.4084,\n",
      "          1.3371],\n",
      "        [ 0.1509, -0.0057,  0.4283,  0.2003,  1.4313,  0.0736, -2.1405,  0.4456,\n",
      "          0.9136],\n",
      "        [ 0.3982, -0.1183, -0.1963, -0.1090,  0.3625,  0.3085,  0.6208,  0.1676,\n",
      "         -1.6522],\n",
      "        [ 0.3634, -0.0222,  0.3720, -0.4313, -1.9413,  0.0894,  2.0099, -0.1821,\n",
      "          1.6395],\n",
      "        [ 0.2280,  0.0465,  0.3112,  0.0046,  0.9994,  0.2072,  0.6061,  0.0322,\n",
      "         -2.1185],\n",
      "        [ 0.9336,  0.0094,  0.4142,  0.0536, -1.2702,  0.2503,  0.6887, -0.3625,\n",
      "          1.0080],\n",
      "        [-1.4663,  0.1312,  1.1341,  0.1901, -0.1790, -0.3372,  2.0218,  0.3724,\n",
      "         -0.7193],\n",
      "        [-0.8291, -0.0664, -0.8078,  0.0539, -0.1142,  0.3236, -0.4433,  0.3492,\n",
      "          1.8838],\n",
      "        [-1.5865,  0.3823,  1.2157, -0.4428, -1.5047, -0.1548,  0.5569,  0.3055,\n",
      "          0.7105],\n",
      "        [ 1.1894, -0.0372,  1.1922,  0.4404,  1.0860, -0.4107,  0.7825, -0.1814,\n",
      "         -1.3075],\n",
      "        [ 0.5245, -0.3173,  1.1924, -0.0447,  0.5055, -0.2817, -2.8720,  0.4040,\n",
      "          0.8846],\n",
      "        [ 1.3810,  0.2876,  0.7018,  0.2342,  1.7903,  0.1436, -2.2629, -0.0846,\n",
      "          1.4566],\n",
      "        [ 0.9692,  0.1753,  0.9520,  0.3816,  0.3412,  0.2461,  1.0320, -0.0231,\n",
      "         -1.2935],\n",
      "        [ 0.8376,  0.0996,  0.7553,  0.0274, -2.0020,  0.4170,  1.9883,  0.4093,\n",
      "          0.9863],\n",
      "        [ 1.1646, -0.1319,  0.8139, -0.1060,  0.4128,  0.3532,  0.6505, -0.2134,\n",
      "         -0.4333]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.5826, -0.1692, -0.7823,  0.8069,  0.9311],\n",
      "        [ 0.8480, -0.7189, -0.9263, -0.2171,  1.1869],\n",
      "        [ 1.0196,  0.2323, -0.9852,  0.2299,  1.0658],\n",
      "        [ 0.3308,  1.9266,  0.0097,  1.5689,  0.4383],\n",
      "        [ 1.2951, -0.3140, -0.2359,  0.0517,  1.3417],\n",
      "        [ 0.7348, -1.5360, -0.9462, -1.6298,  0.6655],\n",
      "        [ 0.4077,  0.9120,  0.1714,  0.3711, -0.1817],\n",
      "        [ 0.9167, -0.6482, -0.3749, -0.0983,  0.4726],\n",
      "        [ 0.1259,  0.5102,  0.3465, -0.3945, -0.1296],\n",
      "        [ 0.0459,  0.4024,  0.0722, -0.1235,  0.3205],\n",
      "        [ 0.6365, -1.4741, -0.1315, -0.8690, -0.1335],\n",
      "        [ 0.1768,  0.6298, -0.3947,  1.3348,  0.4079],\n",
      "        [ 0.1618,  0.4585,  0.6466, -0.1694,  0.5491],\n",
      "        [-1.3140,  1.0143,  0.3129,  1.3963, -0.2996],\n",
      "        [ 0.0792,  0.2585,  0.0931, -0.1065, -0.1611],\n",
      "        [ 1.0024,  0.9906, -0.8161,  1.4583,  0.9868],\n",
      "        [ 0.6070, -1.9796, -0.9356, -2.2019,  1.0458],\n",
      "        [ 0.6096, -0.4865, -0.7567, -0.5530,  1.1331],\n",
      "        [ 0.9005,  1.6566,  0.1690,  1.4107,  0.7809],\n",
      "        [ 1.1190, -0.5494, -1.0298,  0.0051,  1.5875]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.5098,  1.3287,  0.9928,  1.0703,  0.8495,  0.6793,  0.3209,  0.1416,\n",
      "        -0.0823,  0.4781,  0.9051,  0.1045, -0.3242,  0.3114,  0.2742,  0.4315,\n",
      "         0.9190,  0.9654,  1.1800,  0.6483], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.5084,  0.7234,  0.9718,  0.9540,  0.8729,  0.2571,  0.3669,  0.1574,\n",
      "        -0.0211,  0.1432,  0.4539,  0.0371, -0.6273, -0.2832,  0.0923,  0.9835,\n",
      "         0.7331,  1.0828,  1.0557,  0.8314], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for parameter in model.parameters():\n",
    "    print(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 9])\n",
      "torch.Size([9])\n",
      "torch.Size([20, 9])\n",
      "torch.Size([20, 5])\n",
      "torch.Size([20])\n",
      "torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "for parameter in model.parameters():\n",
    "    print(parameter.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
