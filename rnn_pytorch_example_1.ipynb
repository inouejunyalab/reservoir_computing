{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. データの前処理及び変数、関数の宣言"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "sentences = [\"i like dogs\", \"i love coffee\", \"i hate milk\", \"you like cats\", \"you love milk\", \"you hate coffee\"]\n",
    "dtype = torch.float\n",
    "\n",
    "\"\"\"\n",
    "Word Processing\n",
    "\"\"\"\n",
    "word_list = list(set(\" \".join(sentences).split()))\n",
    "word_dict = {w: i for i, w in enumerate(word_list)}\n",
    "number_dict = {i: w for i, w in enumerate(word_list)}\n",
    "n_class = len(word_dict)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "TextRNN Parameter\n",
    "\"\"\"\n",
    "batch_size = len(sentences)\n",
    "n_hidden = 5  # \n",
    "\n",
    "def make_batch(sentences):\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "\n",
    "    for sen in sentences:\n",
    "        word = sen.split()\n",
    "        input = [word_dict[n] for n in word[:-1]]\n",
    "        target = word_dict[word[-1]]\n",
    "\n",
    "        input_batch.append(np.eye(n_class)[input])  # One-Hot Encoding\n",
    "        target_batch.append(target)\n",
    "  \n",
    "    return input_batch, target_batch\n",
    "\n",
    "input_batch, target_batch = make_batch(sentences)\n",
    "input_batch = torch.tensor(input_batch, dtype=torch.float32, requires_grad=True)\n",
    "target_batch = torch.tensor(target_batch, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. RNNの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextRNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.3)\n",
    "        self.W = nn.Parameter(torch.randn([n_hidden, n_class]).type(dtype))\n",
    "        self.b = nn.Parameter(torch.randn([n_class]).type(dtype))\n",
    "        self.Softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, hidden, X):\n",
    "        X = X.transpose(0, 1)\n",
    "        outputs, hidden = self.rnn(X, hidden)\n",
    "        outputs = outputs[-1]\n",
    "        model = torch.mm(outputs, self.W) + self.b\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoheokkim/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0100 cost = 0.350794\n",
      "Epoch: 0200 cost = 0.102215\n",
      "Epoch: 0300 cost = 0.019897\n",
      "Epoch: 0400 cost = 0.011887\n",
      "Epoch: 0500 cost = 0.008380\n"
     ]
    }
   ],
   "source": [
    "model = TextRNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(500):\n",
    "    hidden = torch.zeros(1, batch_size, n_hidden, requires_grad=True)\n",
    "    output = model(hidden, input_batch)\n",
    "    loss = criterion(output, target_batch)\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "  \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D. 検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'like'], ['i', 'love'], ['i', 'hate'], ['you', 'like'], ['you', 'love'], ['you', 'hate']] -> ['dogs', 'coffee', 'milk', 'cats', 'milk', 'coffee']\n"
     ]
    }
   ],
   "source": [
    "hidden = torch.zeros(1, batch_size, n_hidden, requires_grad=True)\n",
    "predict = model(hidden, input_batch).data.max(1, keepdim=True)[1]\n",
    "print([sen.split()[:2] for sen in sentences], '->', [number_dict[n.item()] for n in predict.squeeze()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.9252,  0.4302,  0.8146,  0.1937,  1.7612, -1.1401,  0.7427, -3.9068,\n",
      "         -0.2481],\n",
      "        [ 2.5777,  2.5360,  0.6304,  1.4458, -1.6813,  0.7923, -1.2673, -2.3757,\n",
      "          1.3631],\n",
      "        [-2.4550,  0.6091, -0.2770,  0.8030, -2.6449,  1.0009,  0.1298,  2.4856,\n",
      "          0.2149],\n",
      "        [-2.5399,  1.0224,  0.7613,  0.1300,  1.0377,  1.5288, -1.6626,  2.1457,\n",
      "          1.1803],\n",
      "        [-0.6117,  0.3769,  0.9212,  0.1197, -1.3957,  1.1589,  1.9599, -2.7997,\n",
      "          0.8009]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0787,  0.1266, -0.3946, -1.9833, -1.1144, -2.2162,  0.9435,  1.2432,\n",
      "        -0.6966], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0998, -0.7603, -0.2004,  1.9505,  0.3418, -0.3627, -0.4216, -0.2708,\n",
      "         -0.2881],\n",
      "        [ 0.1023, -0.8960,  0.6622, -1.1051,  0.0143,  0.3702, -0.1699,  0.2059,\n",
      "         -0.2099],\n",
      "        [ 0.2888, -1.9985,  0.0397,  1.5349,  0.0796,  0.7711,  0.1819, -0.1057,\n",
      "         -0.5852],\n",
      "        [ 0.3120,  1.0495, -1.2279,  0.8978, -0.3129, -1.1208,  0.2949,  0.0872,\n",
      "          0.7072],\n",
      "        [ 0.0196, -0.8148,  1.3701, -0.9244,  0.1600, -0.8623, -0.4341, -0.1816,\n",
      "          1.1612]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.4403, -0.4034, -0.4841, -1.7435, -1.0586],\n",
      "        [ 0.3329,  0.0092, -0.7914,  0.2643,  1.2752],\n",
      "        [ 0.4182,  0.7585,  0.8027, -1.4983, -1.4430],\n",
      "        [ 0.2902, -0.1384,  0.0639, -0.2060,  0.0422],\n",
      "        [ 0.1307,  0.3284,  0.8253, -0.2127, -0.4398]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1755, -0.7495, -0.7807,  0.0056, -0.5436], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0062, -0.7105, -0.2745,  0.0321, -0.0160], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for parameter in model.parameters():\n",
    "    print(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 9])\n",
      "torch.Size([9])\n",
      "torch.Size([5, 9])\n",
      "torch.Size([5, 5])\n",
      "torch.Size([5])\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "for parameter in model.parameters():\n",
    "    print(parameter.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
